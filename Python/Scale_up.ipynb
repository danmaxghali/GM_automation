{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67acbf59-abd0-4318-baeb-7f98979b9d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb3d4s.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2rh1.ent' \n",
      "Structure exists: 'C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\\pdb2r4r.ent' \n",
      "[3d4s] STDOUT: 2025-03-04 10:21:41,613 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-04 10:21:41,613 | INFO : Done in 2.74 seconds\n",
      "\n",
      "2025-03-04 10:21:41,613 | INFO : Extracted zip file to: output/\n",
      "2025-03-04 10:21:41,613 | INFO : Done in 2.74 seconds\n",
      "\n",
      "[3d4s] STDERR: \n",
      "[2rh1] STDOUT: 2025-03-04 10:21:53,758 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-04 10:21:53,759 | INFO : Done in 2.76 seconds\n",
      "\n",
      "2025-03-04 10:21:53,758 | INFO : Extracted zip file to: output/\n",
      "2025-03-04 10:21:53,759 | INFO : Done in 2.76 seconds\n",
      "\n",
      "[2rh1] STDERR: \n",
      "[2r4r] STDOUT: 2025-03-04 10:21:59,058 | INFO : Extracted zip file to: output/\n",
      "\n",
      "2025-03-04 10:21:59,058 | INFO : Done in 1.66 seconds\n",
      "\n",
      "2025-03-04 10:21:59,058 | INFO : Extracted zip file to: output/\n",
      "2025-03-04 10:21:59,058 | INFO : Done in 1.66 seconds\n",
      "\n",
      "[2r4r] STDERR: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from Bio import PDB, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pydssp\n",
    "\n",
    "\n",
    "# === 1️⃣ Parallel PDB Downloading ===\n",
    "def download_pdb(pdb_code, save_dir):\n",
    "    \"\"\"Downloads a PDB file in parallel.\"\"\"\n",
    "    pdb_file = PDB.PDBList()\n",
    "    pdb_file.retrieve_pdb_file(pdb_code, file_format=\"pdb\", pdir=save_dir, overwrite=False)\n",
    "\n",
    "\n",
    "def download_all_pdbs(pdb_codes, save_dir):\n",
    "    \"\"\"Downloads multiple PDB files in parallel.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(download_pdb, pdb_codes, [save_dir] * len(pdb_codes))\n",
    "\n",
    "\n",
    "# === 2️⃣ Amino Acid Conversion ===\n",
    "AA_DICT = {\n",
    "    \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "    \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "    \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "    \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\"\n",
    "}\n",
    "\n",
    "\n",
    "def three_to_one(resname):\n",
    "    \"\"\"Converts 3-letter residue name to 1-letter code.\"\"\"\n",
    "    return AA_DICT.get(resname, \"X\")  # 'X' for unknown residues\n",
    "\n",
    "\n",
    "# === 3️⃣ Extract FASTA from PDB ===\n",
    "def extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir):\n",
    "    \"\"\"Extracts the sequence of a specific chain from a PDB file and writes it as FASTA.\"\"\"\n",
    "    pdb_filepath = os.path.join(pdb_dir, f\"pdb{pdb_code}.ent\")\n",
    "    fasta_filepath = os.path.join(fasta_dir, f\"{pdb_code}.fasta\")\n",
    "\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "\n",
    "    # Only process first model (speed optimization)\n",
    "    model = structure[0]\n",
    "    sequence, observed_residues = [], []\n",
    "\n",
    "    chain = model[chain_id] if chain_id in model else None\n",
    "    if chain:\n",
    "        for residue in chain.get_residues():\n",
    "            if PDB.is_aa(residue):\n",
    "                sequence.append(three_to_one(residue.get_resname()))\n",
    "                observed_residues.append(residue.id[1])\n",
    "\n",
    "    # Write FASTA\n",
    "    fasta_seq = SeqRecord(Seq(\"\".join(sequence)), id=f\"{pdb_code}_{chain_id}\", description=\"\")\n",
    "    SeqIO.write(fasta_seq, fasta_filepath, \"fasta\")\n",
    "\n",
    "    return observed_residues\n",
    "\n",
    "\n",
    "# === 4️⃣ Run DeepTMHMM Asynchronously ===\n",
    "def run_deeptmhmm(pdb_code, fasta_filepath, results_dir):\n",
    "    \"\"\"Runs DeepTMHMM for a single PDB code asynchronously.\"\"\"\n",
    "    pdb_results_dir = os.path.join(results_dir, pdb_code)\n",
    "    os.makedirs(pdb_results_dir, exist_ok=True)\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        [\"biolib\", \"run\", \"DTU/DeepTMHMM\", \"--fasta\", fasta_filepath],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        cwd=pdb_results_dir,\n",
    "        start_new_session=True  # Allows independent execution\n",
    "    )\n",
    "    return process\n",
    "\n",
    "\n",
    "def run_all_deeptmhmm(pdb_codes, fasta_dir, results_dir):\n",
    "    \"\"\"Runs DeepTMHMM for multiple PDB codes in parallel asynchronously.\"\"\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        processes = {\n",
    "            pdb_code: executor.submit(run_deeptmhmm, pdb_code, os.path.join(fasta_dir, f\"{pdb_code}.fasta\"), results_dir)\n",
    "            for pdb_code in pdb_codes\n",
    "        }\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    for pdb_code, future in processes.items():\n",
    "        process = future.result()\n",
    "        stdout, stderr = process.communicate()\n",
    "        print(f\"[{pdb_code}] STDOUT:\", stdout)\n",
    "        print(f\"[{pdb_code}] STDERR:\", stderr)\n",
    "\n",
    "\n",
    "def keep_only_tmr(results_dir, pdb_codes):\n",
    "    \"\"\"Removes all files except TMRs.gff3 in each PDB results folder.\"\"\"\n",
    "    for pdb_code in pdb_codes:\n",
    "        pdb_results_dir = os.path.join(results_dir, pdb_code, \"biolib_results\")  # Ensure correct subfolder\n",
    "\n",
    "        if os.path.exists(pdb_results_dir):\n",
    "            for filename in os.listdir(pdb_results_dir):\n",
    "                file_path = os.path.join(pdb_results_dir, filename)\n",
    "\n",
    "                if filename != \"TMRs.gff3\":\n",
    "                    try:\n",
    "                        os.remove(file_path)  # Remove only files, leave the folder\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not remove {file_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# === 5️⃣ Define Directories and Execute ===\n",
    "pdb_codes = [\"3d4s\", \"2rh1\", \"2r4r\"]\n",
    "pdb_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\PDB_files\"\n",
    "fasta_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\Fasta_files\"\n",
    "results_dir = r\"C:\\Users\\Student\\OneDrive - Aston University\\Documents\\Biology\\Project\\Project_automation\\Python\\DeepTMHMM_results\"\n",
    "\n",
    "# Download PDB files in parallel\n",
    "download_all_pdbs(pdb_codes, pdb_dir)\n",
    "\n",
    "# Extract sequences\n",
    "chain_id = \"A\"\n",
    "all_observed_residues = {\n",
    "    pdb_code: extract_pdb_fasta(pdb_code, pdb_dir, chain_id, fasta_dir) for pdb_code in pdb_codes\n",
    "}\n",
    "\n",
    "# Run DeepTMHMM in parallel asynchronously\n",
    "run_all_deeptmhmm(pdb_codes, fasta_dir, results_dir)\n",
    "keep_only_tmr(results_dir, pdb_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0ddc88-afbb-474e-8bbb-eb8cf83aaae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 371 is out of bounds for axis 0 with size 216",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 73\u001b[0m\n\u001b[0;32m     68\u001b[0m         extended_tmh_ranges\u001b[38;5;241m.\u001b[39mappend((extended_start, extended_end))\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extended_tmh_ranges\n\u001b[1;32m---> 73\u001b[0m extended_tmh_ranges \u001b[38;5;241m=\u001b[39m \u001b[43mextend_tmh_ranges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmh_ranges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(extended_tmh_ranges)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Convert TMH positions to actual PDB residue numbers as pairs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 54\u001b[0m, in \u001b[0;36mextend_tmh_ranges\u001b[1;34m(tmh_ranges, ss_data, max_extend)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_extend \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     53\u001b[0m     prev_res \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m-\u001b[39m i\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_res \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mss_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprev_res\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# Ensure valid index\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         extended_start \u001b[38;5;241m=\u001b[39m prev_res\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 371 is out of bounds for axis 0 with size 216"
     ]
    }
   ],
   "source": [
    "tmh_data = {}\n",
    "pdb_data = {}\n",
    "\n",
    "# Align output ranges with actual observed residues\n",
    "for pdb_code in pdb_codes:\n",
    "    tmh_ranges = []\n",
    "    tmh_result_file = f\"C:\\\\Users\\\\Student\\\\OneDrive - Aston University\\\\Documents\\\\Biology\\\\Project\\\\Project_automation\\\\Python\\\\DeepTMHMM_results\\\\{pdb_code}\\\\biolib_results\\\\TMRs.gff3\"\n",
    "    \n",
    "    with open(tmh_result_file) as file:\n",
    "        for line in file:\n",
    "            if \"TMhelix\" in line:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                start, end = int(parts[2]), int(parts[3])\n",
    "                tmh_ranges.append((start, end))\n",
    "    \n",
    "    # Use DSSP\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(pdb_code, pdb_filepath)\n",
    "    \n",
    "    # Select the first structure in the file and the A chain\n",
    "    chain = structure[0]['A']\n",
    "    \n",
    "    # Retrieve coordinates\n",
    "    coordinates = []\n",
    "    for residue in chain:\n",
    "        if PDB.is_aa(residue):\n",
    "            res_name = residue.get_resname()\n",
    "            for atom in residue:\n",
    "                if atom.get_name() in ['N', 'CA', 'C', 'O']:\n",
    "                    coordinates.append(atom.coord)\n",
    "    \n",
    "    # Prepare eliments of pytorch tensor\n",
    "    L = sum(1 for residue in chain if PDB.is_aa(residue))\n",
    "    atoms = 4\n",
    "    xyz = 3\n",
    "    \n",
    "    # Make coordinates into array first to increase efficiency\n",
    "    coord_array = np.array(coordinates)\n",
    "    \n",
    "    # Create tensor\n",
    "    coord_tensor = torch.tensor(coord_array, dtype=torch.float32).reshape([L, atoms, xyz])\n",
    "    \n",
    "    # Use pydssp to get secondary structure\n",
    "    ss_data = pydssp.assign(coord_tensor, out_type='c3')\n",
    "\n",
    "    def extend_tmh_ranges(tmh_ranges, ss_data, max_extend=9):\n",
    "        extended_tmh_ranges = []\n",
    "    \n",
    "        for start, end in tmh_ranges:\n",
    "            # Extend start position backwards if residues are consecutive 'H'\n",
    "            extended_start = start\n",
    "            for i in range(1, max_extend + 1):\n",
    "                prev_res = start - i\n",
    "                if prev_res >= 0 and ss_data[prev_res] == \"H\":  # Ensure valid index\n",
    "                    extended_start = prev_res\n",
    "                else:\n",
    "                    break  # Stop if a non-'H' is encountered\n",
    "    \n",
    "            # Extend end position forwards if residues are consecutive 'H'\n",
    "            extended_end = end\n",
    "            for i in range(1, max_extend + 1):\n",
    "                next_res = end + i\n",
    "                if next_res < len(ss_data) and ss_data[next_res] == \"H\":  # Ensure valid index\n",
    "                    extended_end = next_res\n",
    "                else:\n",
    "                    break  # Stop if a non-'H' is encountered\n",
    "    \n",
    "            extended_tmh_ranges.append((extended_start, extended_end))\n",
    "    \n",
    "    \n",
    "        return extended_tmh_ranges\n",
    "    \n",
    "    extended_tmh_ranges = extend_tmh_ranges(tmh_ranges, ss_data)\n",
    "    print(extended_tmh_ranges)\n",
    "\n",
    "    # Convert TMH positions to actual PDB residue numbers as pairs\n",
    "    tmh_extended_pairs = [\n",
    "        (observed_residues[start - 1], observed_residues[end - 1]) for start, end in extended_tmh_ranges\n",
    "    ]\n",
    "\n",
    "    def reorder_gpcr_tmh_ends(tmh_extended_pairs):\n",
    "        pattern = [\"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\", \"intra\", \n",
    "                   \"extra\", \"extra\", \"intra\", \"intra\", \"extra\", \"extra\", \"intra\"]\n",
    "    \n",
    "        reordered = []\n",
    "        for i, label in enumerate(pattern):\n",
    "            if label == \"extra\":\n",
    "                reordered.append(tmh_extended_pairs[i // 2][0])  # Take start residue\n",
    "            else:  # \"intra\"\n",
    "                reordered.append(tmh_extended_pairs[i // 2][1])  # Take end residue\n",
    "    \n",
    "        return reordered\n",
    "    \n",
    "    flattened_tmh = reorder_gpcr_tmh_ends(tmh_extended_pairs)\n",
    "    \n",
    "    pdb_data[pdb_code.upper()] = {\"A\": flattened_tmh}\n",
    "\n",
    "\n",
    "print(pdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9e879-9996-48ce-8ab7-a9139a46b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
